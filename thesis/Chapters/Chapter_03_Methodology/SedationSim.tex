%! suppress = EscapeUnderscore

\section{Data Analysis}\label{sec:data-analysis}
The NMM implemented in this thesis is configured to simulate data with a step-size of $\SI{1}{\milli\second}$,
yielding a $\SI{1000}{\hertz}$ signal.
The data generated by NMMs has a clear advantage to real EEG recordings:
It has no missing samples,
was never recorded with insufficient impedance,
and there are no artifacts or excessive noise.
That means there is nothing degrading the signal quality,
which would require post-processing steps to try to mitigate these problems
before further analysis.
Nonetheless, it is important to understand the properties of the simulated data to correctly work with it.

In a complex system, one can expect transient behavior;
When starting a run, with all its state-variables uninitialized,
the system reacts with high-amplitude oscillations to the "disturbance" of the random input.
Usually, it stabilizes quickly and then exhibits the expected behavior.
Thus, the initial section of simulated data,
i.e., before the transient decays,
should be discarded before further analysis (stabilization-time may vary greatly with parameterization).
This is not unlike to some physical EEG recording hardware,
were the built-in filters require time to initialize before they produce a usable signal.

For the actual analysis of the data,
we will require the frequency information of the signal,
as most of our analysis occurs in the frequency domain.
A segment of EEG-activity can be analyzed by calculating its
Power Spectral Density (PSD),
which contains information about the frequency distribution within the segment,
but discards any information about the time of occurrence.
This is helpful when determining general information about a signal that is more or less stationary,
as shown in Fig.~\ref{fig:jr_psd} or Fig.~\ref{fig:df_psd}.
However, when working with signals that contain variations in their distribution over time,
which is the expectation for our simulation with varying parameterization and for real EEG signals in general,
it is crucial to know that the signal changes and at what point in time the changes occur.
An obvious approach is to cut the signal into smaller pieces,
analyze those individually and work with these discrete `snapshots` of frequency distribution.
To increase time-resolution, one might be tempted to minimize the size of these signal-windows,
however, this comes with the caveat,
that shorter segments do not contain enough data to detect lower frequencies,
diminishing frequency-resolution.
This dilemma is called `time-frequency-tradeoff`.
There are multiple techniques that aim to mitigate this problem.
The approach that will be used in this thesis uses a window-size that is large enough to maintain a reasonable
frequency distribution, but the segments are chosen to contain a large degree of overlap.
This does come with its own issues, as overlapping windows introduce a certain amount of redundancy in the result,
but provides a more than acceptable compromise for our application.
The resulting data is normally visualized in so called `spectrograms`,
that plot time and frequency on the x- and y-axis,
and encode the respective frequency intensity with a colormap (e.g., Fig.~\ref{fig:jr_spec}
and Fig.~\ref{fig:df_spec}).
Further details to these standard signal-processing techniques can be
found in the literature (e.g.,~\cite{mecarelli_clinical_2019}).

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \pgfplotsset{
        %% Axis
            scale only axis,
            width=0.4\linewidth,
            height=4cm,
            every axis/.append style={
                line width=1pt,
                tick style={line width=0.8pt},
                %   grid style={dashed, black!20},
                %  grid=major,
            },
%               %% X-Axis
            xmin=-0.1,
            xmax=5,
        }
        \begin{groupplot} [
                group style={
                    group size=2 by 2,
                    vertical sep=12mm,
                    horizontal sep=12mm,
                },
                yticklabel style={
                    /pgf/number format/fixed,
                    /pgf/number format/precision=2
                },
                legend style={nodes={scale=0.8, transform shape}, thin},
                legend image post style={scale=0},
            ]
            \nextgroupplot[ylabel=$\SI{}{\milli\volt}$, xlabel=$s$]
            \addplot [line width=.5pt,solid, cyan]
            table[x=x,y=y ,col sep=comma]{data/methodology/uncut.csv};
            \legend{\textbf{(a)} raw data};

            \nextgroupplot[xmin=2.0, xlabel=$s$]
            \addplot [line width=.5pt,solid, cyan]
            table[x=x,y=y ,col sep=comma]{data/methodology/uncut.csv};
            \legend{\textbf{(b)} data with transients removed};
        \end{groupplot}
    \end{tikzpicture}

    \caption{
        \textbf{Processing of simulated data.} \\
        Removing signal transients by cutting off the first $\SI{2}{\second}$ of the
        data (generated by JR Model).
    }
    \label{fig:initial_oscilations}
\end{figure}